

Containers - A container is a standard unit of software that packages up code and all it's [[dependencies]] so the application runs quickly and reliably from 1 computing environment to another. Container is the process which is executing your application code with it all dependencies. How will this particular code execute? Because once you will execute the container, container will consume a base image. This is called a "container image". The container image contains the your project code plus all its dependencies plus and basic operating system to execute that particular process. 

**Recap this section** - The "container image" aka your "base image" contains your project code along with all the dependencies needed so the application can run along with the operating system to carry out this particular process. Once you execute the container which houses the base image or container image this is how the code executes. A container is a standard unit of software that packages up all the code/dependencies needed so the application runs fast and reliably from 1 computing environment to another one. Container is the process which is executing your app code along with all the dependencies and the os. 

![[Pasted image 20240627150130.png]]

The first statement is saying the container is a standard unit of a software that package your code and all it's dependencies. What do you need to do? You need to create a base image that will contain your (os), that will contain your software code, plus all its dependencies, plus a basic unit of the software (os). Right and then we're saying so that application runs quickly, reliably from one computer environment to another computer environment. As in real life, if you have initiated some process, you terminated that process, then you can initiate that particular process. It matters not that you are initiating that particular process on the same OS, on the same computer on which it was started earlier, or you're initiating that particular process on some other OS because you have all your application code and dependencies on another OS as well where you are executing your another application instance.


So we're saying application learns quickly, reliably from one computer environment to another computer environment. When we a complete package, when we have the image which contains our application code, all its dependencies plus a basic (os), then I can execute that particular image on any OS, right on any computer. So that's the beauty of the container. That container will build up all the things together and then execute these things as a single process within the (os).

(Recap Above)- When a container executes it's only because we have the code, os and dependencies needed and you can execute said container on any os. This is the beauty of it. Cost efficiency also plays a role in containers as they're created as needed and then discarded when you don't thus freeing up resources and improve billing accuracy.

[Example of Containers & benefits]- Suppose you are working on a web app and that web app requires Tomcat to start. Senior development machine what do you need to do? You need a basic os that can execute the Tomcat. You need the Tomcat dependency, the package and all the dependencies of the Tomcat, and you need your  application code which will be executed by the Tomcat, right? Suppose you have developed that particular stuff. Now you hand over that particular stuff to the quality assurance team and the tester will qualify your development.

So what the tester need to do? Tester needs to setup the complete Tomcat on his machine. Tester needs to set the complete software code on his machine. Right? Then the tester would be able to execute that particular step and test that particular which you have developed right now after the tester qualification that is going to the production. In the production you need to again repeat the same step. You need to read in the environment, you need to install the Tomcat, install the dependencies, you need to get all the application code and then you need to execute that particular thing. This is a very time consuming/lengthy process.

With the help of the container, what do you need to do, the developer needs to package their application code and all its dependencies. In the dependencies it includes everything like Tomcat. The dependencies of the Tomcat, the operating system and everything. They will create a bundle which is called a "docker image". Once the bundle will be created, that particular image will be executed with the help of the Docker. So, what the tester needs to do is just need to download or get that image from the repository where you have uploaded it and execute that particular image with the help of a simple command which is being handled by the developer.

Same thing will need to execute on the production as well. So here what we are achieving. We are achieving the less human errors, because when you will setup something manually, there's a chance of human errors and you missing something, you miss some dependencies, and that will cause the failures. Right? Because everything is backed up in an image and that image is built as a bundle. So the same image can be delivered to the way the same image can be delivered to the other development team. The same image can be delivered to the production or anywhere we want to execute your application.

So these are the very high level and very basic idea that how containers work. Let's understand a very simple process. Now, in the Google Cloud, how we are executing our process. We're creating a virtual machine, right? That is a Google cloud, virtual machine that is bein built on some basic operating system that have their own hypervisor. Then they have their own network config and other stuff. Suppose we have a 32 GB available hardware. When we create a vm we're defining some memory right? So we are defining a memory like 12 GB virtual machine with some code of CPU i.e 8GB Used and + 4GB Free. When we are executing a particular process on that particular machine, we can say VM one, then that process is taking only 8GB in use at rest 4 GB is ideal for me right? The 4GB not being used by my app which is running on that particular virtual machine. Only 8 GB and less than eight GB is consumed by my application.

In the same way I have created another machine. The machine number 2 and I have allocated that particular machine 8 GB Ram right? That eight GB Ram is allocated to another VM which we call VM 2 and VM two is executing some another process. Out of the 8 GB, my app is just consuming the six GB is still the two GB Ram is free for the use. That is no more in use by the app. That is free = ideal RAM. In the Third Way, I have created another VM write, and in that particular VM I have. And on that particular VM, I've executed my application. My app is just taking 4 GB, right? That isn't taking. So, in that case, eight GB memory is free. What we are trying to do is, we have some hardware which have the fixed limit and we're putting virtual machines on that particular hw with different different configuration for different different application.

And what they are and what the applications are doing? they application are executing on each VM right? They have their different memory requirement and the CPU requirement. But in that particular case we have 2 scenarios. Either the app can consume less memory or either the app can consume more memory which we allocated to the virtual machine. In that case, if memory is less, which is always the case in 99.99%, every software developer execute the application with extra money and extra CPU on the virtual machine. Suppose your application just need 100 MP or 200 MP ram for its processing. But but still developer will take at least four GB ram virtual machine. Because, if sometime application need more memory, application needs a more CPU so that it can consume. But in ideal case, you're 90%, 80% memory are in no use. That is free for that is free. Right, your application isn't consuming that particular memory. So, the same is the case over here. We are creating the different, different VMs. These different different VMs are executing different, different applications and these applications have their own memory and on CPU consumption, right? So in that particular case, what we are doing, we are basically wasting the resource and on cloud resource is everything because every resource you will consume, you have to pay for that particular resource, right? 

So in cloud, in the era of cloud technology resources, the money. So by that particular way, if you are opting out this particular model, you are wasting your hard earned money right? Every organization which is moving in that particular direction, they have the server stick application.
If they can containerized that particular application, then they must containerized that particular application and we will see that how container will help us to avoid such kind of scenarios, avoid
such kind of scenarios, avoid such kind of waste of resources, avoid such kind of vistas of money. Right over here I have taken example of the memory. The same example can be applicable with the CPU as well, right? The base hardware can have the 100 core CPU you have divided your VM like 2016 CPU eight CPU nine CPU. But the operating system is just taking 0.1%, 0.5% poin 8%, point 10% CPU of that particular VM. The rest of the CPU 99% of the CPU, 80% of the CPU, 70% of the CPU is free for use. That is idle. No one using that particular CPU. But still you are paying for that particular resource which you are consuming on the Google Cloud or any cloud you're working.

Right? So what's the solution? Solution is the container while the container is the solution. Why is the container the solution? Let's see. Suppose we have in 32 GB hardware and we are again dividing that particular 32 GB hardware to the different containers. Right? We are creating a container and container which is being initiated with the eight GB, six GB and ten GB. That particular GB will be consumed by the container, right? Which is executing the application as a process, not a VM and process. Always take the sufficient memory which is in use from the base OS. I mean to say over here we don't have any middle layer or the separate operating system or the virtual machine which is executing that particular process. No, the F1 is a process which is executing in a container which have the HGV memory configuration. When I 'm saying HGV memory configuration, I'm not hardcoded in the memory for the container. Once your application will start, the memory which is required for that particular application to start and execute the container will automatically allocate that particular memory from the base hardware system. Right? In that particular case, at one time your application one is taking HGV.

After a few hours it is taking two GB. Then container will release the rest of the six GB machine which is being allocated by the base so as to contain the application start time. So by this particular way we are saving the memory, we are saving the resource because container will only demand the memory which is required to execute the process node to complete VM. WE don't have hardcoded configuration for the memory for the CPU. Right? They are the dynamic fields container will demand the memory as per the application. Need write note as per the defined configuration. So at runtime, if application need to GB container will get the two GB from the base. If application need ten GB container will get the tangible from the base. So, in that particular case we can see that we are executing three application in the container and these three applications are taking the different different memory from the base hardware, which is eight GB, six 6B and ten GB.

![[Pasted image 20240627162021.png]]

So over here we are consuming only 24 GB and rest. Eight GB is still allocated for the other containers and rest. Eight GB is still available on the base hardware for other containers. Right? So you can spin up the other containers in that particular eight GB machine. The same concept applies with the CPU as well. Write the container application, get the CPU which is required to execute. It will not get the CPU which is hardcoded on that particular virtual machine. Right. 

Let's try to understand this particular thing in a more detailed way. So we are taking over here to tables the virtual machine in the Docker containers. Right? The best thing is both the virtual machine and the Docker container will have the host operating system. Right? Because without a host operating system, nothing will work. We will also work on the host operating system. Also, the Docker will also work on the host operating system containers generally executed by the Docker. Right? So Docker is a engine or you can say Docker is a facility which will execute the containers on the machine, right? So, basic requirement is both have the cost OS. In the case of the virtual machine, when you will create the virtual machine, every virtual machine will create a guest operating system on the host OS. So host OS is working. That is, that could be the windows, that could be the Linux Unix or anything , right? Then you're creating the virtual machine. Virtual machine will have their own guest OS, right? If you own your laptop on your local servers, if you will create the multiple virtual machine, then you can see that on the Linux host machine, you can execute a Windows virtual machine. So how Windows is executing on the Linux machine, because in your virtual machine, a complete OS a complete Windows OS is enabled on the Linux box. 

We can see that if you will talk about servers, we will have the Windows server, we will have the Windows server, we will have the Linux server. Our Windows Server are executing on the Linux server because they have created their own environment. This is a separate VM. The same thing is happening in the cloud as well. Every virtual machine will have their own guest OS, which is working on the best OS kernel. That guest with is executing the app with the help of the binaries and the libraries, which is specific to your application. Suppose we have the two applications application one and application two. They both have their own binaries and rows and own libraries. So in case of virtual machine, what is the stack stack is like? We have the host OS, then we have the guest OS for the application one, then we have the guest OS for the application two, then we have the code for application one and we have the code for application two. But, in the case of the Docker, we have the host operating system, then we have the directly code of the application, one end code of the application. Two, we don't have any middleware which is a guest OS in case of containers. We are not creating any guest operating system on the host operating system.


Once you have your code. Once you have your library, your OS, you can execute your application. 
So we can execute the application when an application to same is being happening on the Docker containers. So in the Docker containers we are avoiding a complete layer which is a guest OS, which is more time consuming and installation and which have the hardcoded defined resource configuration. If you will install VMware on your machine and you will create two separate VMs, then you will see that you have to define a specific hardware to that particular VM, right? You need to define a specific hardware, You have a specific hardware requirement to initiate that particular VM, even if you will install dual OS on your machine , right? You will install the windows or OS as a dual operating system, you need to define a complete  RAM and completed hardware, complete memory and CPU, which that particular is going to use. Right? Same thing is happening with the VM. So in the case of the Docker we are removing that particular guest is to completely right. We'll create an image which will have my application code. The dependencies database is that base OS could be the Linux OS or it could be the anything, right, could be the base community or CentOS or fedora or windows. That particular base OS have very minimal package which is required to execute a process on the operating system. It will tell you don't believe the Alpine Linux version of the Docker is just 10 to 16 MB image. That particular image is a complete OS in itself for the Docker. Alpine Linux version of the operating system, which is used for the Docker image. That is in just 10 to 18MB or 10 to 16 and be in size. I don't remember the exact size, but that is the size which I saw earlier, right? The complete operating system for the Docker. Back then to 18mb. Image will execute on your host OS and that will execute your application.  So this is the benefit of the Docker. So in the traditional way we are creating a new build.

We will have the multiple operating system, we'll have the separate libraries and we will have the heavy resources. That particular thing will be very heavy, it will be more time consuming. But in case of the Docker, we have the new build, obviously the same operating system which is a host operating system. We have the separate libraries, which is very lightweight and less time consuming, if you will, creating a VM that will take a minutes and sometimes that will take several minutes. But if you are creating a Docker container that will take few seconds to spin up, right, I will show you Docker container will spin up in a very few seconds. 

![[Pasted image 20240627194332.png]]

*Kubernetes* (official def)- Kubernetes is an open source container orchestration platform that automates many of the manual processes involved in deploying, managing and scaling containerized applications. 

Kubernetes  is an orchestration system which will execute the containers, right? Kubernetes is an open source container orchestration platform that automate many of the manual process involving deployment of containers. Management of the containers is scaling and scaling of the containers application. We can't see the complete details of the deployment, managing and scaling and scaling of the containerization in this particular course/lecture. If you are really interested in Docker and the Kubernetes, then we have the seperate course which is 14-18 hours long, right in which I have explained every aspect of the Kubernetes and Docker, including the networking, mounting, coding, the deployment management upgrade downgrade. The scaling is scaling. There's a lot of a lot of things which I have explained over there right? If you want to enroll that particular course, you can go through the resource section of this lecture. You will get the URL of that particular course. 

Kubernetes was originally developed by the Google engineers. Google was one of the early contributors to Linux container technology that has talked about publicly how everything can run in the containers. All the containers have their own limitation. To date, you can't execute each and every application in the container right? You cant execute the big table's big data table. You can't execute the technologies like which is working with the analytics and the containers. Till now you can't execute. But I'm not saying that it's not possible. Many organizations are taking the many initiatives and they are executing the analytical application in the container itself. Even though a few months back I have also developed an analytic application inside the container that is working with the Edge base, apache edge base. So this is statement that  the analytical applications are generally not familiar with the containers/not suitable for the containers. That may depend on the technology and the architecture of your application. Ideally, Google has shown that how everything can be executed in the containers, Google generates more than 2 billion containers deployment. 

![[Pasted image 20240627201604.png]]

Can you execute 2 billion virtual machines in the Google or any cloud for your organization? Is it possible? For me hell no. But any cloud. But yes, you can execute more than two and more than 100 million containers on any cloud site. All that's not possible to execute that number of virtual machine on the cloud because every cloud have their own limitation for every customer, they will not allow you to consume that much of resources. They will not allocate that much resources to you. Definitely, you can execute that much number of containers because containers is just a process which is consuming some memory, which is consuming some operating system on the ist is right, if you will take that you are creating a N1 standard one machine which is 3.75 GB in RAM and one core CPU. Right? Your application is executing on that particular. When a standard machine which is taking only 0.1% of CPU and 2% of memory in that particular case, if you are executing that particular application on the container and you have that particular is standard one as you hope and you have that particular and one is standard as a host operating system. So, on the same machine you can execute more than 40, 45 plus containers. More than 45 instances of your application can execute on the same machine where you are executing a single instance of your application on a machine, right? All the hardware configuration is the same, but the technology stack is different. The Google, Kubernetes or the communities is basically powered by the internal platform or the internal tool of the Google engineering, which is called "Borg".





With Kubernetes- You can decide when your containers should run, increase, or decrease the size of application containers or check the resource consumption of your deployment applications. 


